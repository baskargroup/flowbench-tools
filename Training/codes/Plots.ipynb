{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2649cd3a-8974-4545-b865-52e170690896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA for tensor operations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from codes.utils.device import device\n",
    "from codes.models.FNO import TensorizedFNO\n",
    "from codes.models.CNO import CompressedCNO\n",
    "from codes.data.dataset import LidDrivenDataset\n",
    "from codes.utils.visualization import plot_ldc_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a7fccf4-9083-4a6e-9753-2e77fd944e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yaml_file(folder_path, file_name):\n",
    "    \"\"\"Reads a YAML file and returns the data.\"\"\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    with open(file_path, 'r') as file:\n",
    "        try:\n",
    "            data = yaml.safe_load(file)\n",
    "            return data\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(f\"Error reading YAML file: {exc}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6189b6e6-77b2-41c2-94f9-c4c81b41c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_metrics_validation_set(y, yhat, resolution):\n",
    "\n",
    "  half_res = int(resolution/2)\n",
    "\n",
    "  mse = nn.MSELoss()\n",
    "\n",
    "  mses = torch.zeros(7) # we are outputing 7 validation metrics\n",
    "\n",
    "  #mse full\n",
    "  mses[0] = mse(y,yhat)\n",
    "\n",
    "  #mse u,v\n",
    "  mses[1] = mse(y[:,0:2,:,:], yhat[:,0:2,:,:])\n",
    "\n",
    "  #mse p\n",
    "  mses[2] = mse(y[:,2,:,:], yhat[:,2,:,:])\n",
    "\n",
    "  #mse near object u, v\n",
    "  if resolution == 128:\n",
    "    mses[3] = mse(y[:,0:2,25:104,25:104], yhat[:,0:2,25:104,25:104])\n",
    "  elif resolution == 256:\n",
    "    mses[3] = mse(y[:,0:2,51:206,51:206], yhat[:,0:2,51:206,51:206])\n",
    "  else:\n",
    "    mses[3] = mse(y[:,0:2,102:410,102:410], yhat[:,0:2,102:410,102:410])\n",
    "\n",
    "  #mse near object p\n",
    "  if resolution == 128:\n",
    "    mses[4] = mse(y[:,2,25:104,25:104], yhat[:,2,25:104,25:104])\n",
    "  elif resolution == 256:\n",
    "    mses[4] = mse(y[:,2,51:206,51:206], yhat[:,2,51:206,51:206])\n",
    "  else:\n",
    "    mses[4] = mse(y[:,2,102:410,102:410], yhat[:,2,102:410,102:410])\n",
    "\n",
    "  #mse cd\n",
    "  mses[5] = mse(y[:,3,:half_res,:], yhat[:,3,:half_res,:])\n",
    "\n",
    "  #mse cl\n",
    "  mses[6] = mse(y[:,3,half_res:,:], yhat[:,3,half_res:,:])\n",
    "  \n",
    "  #return\n",
    "  return mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70175858-3b81-402b-ad46-fbff567b7fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LidDrivenDataset(file_path_x= '../NS/512x512/harmonics_lid_driven_cavity_X.npz', \n",
    "                           file_path_y = '../NS/512x512/harmonics_lid_driven_cavity_Y.npz')\n",
    "train_loader, val_loader = dataset.create_dataloader(batch_size= 5, split_fraction= 0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed444a50-5acd-4964-a3fd-c82ef90fd96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"filtered_lrelu_plugin\"... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "config_data = read_yaml_file('experiments/cno_1', \"config.yaml\")\n",
    "\n",
    "CNO_trained_1 = CompressedCNO(in_dim = config_data['in_dim'], out_dim = config_data['out_dim'], \n",
    "                          N_layers = config_data['N_layers'], in_size = config_data['in_size'], \n",
    "                          out_size = config_data['out_size']).to(device)\n",
    "\n",
    "CNO_trained_1.load_checkpoint(save_name=\"400\", save_folder='experiments/cno_1/checkpoints')\n",
    "\n",
    "mse_0 = 0\n",
    "mse_1 = 0\n",
    "mse_2 = 0\n",
    "mse_3 = 0\n",
    "mse_4 = 0\n",
    "mse_5 = 0\n",
    "mse_6 = 0\n",
    "\n",
    "CNO_trained_1.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs, targets = batch[0].to(device), batch[1].to(device)\n",
    "        outputs = CNO_trained_1.forward(inputs)\n",
    "\n",
    "        mses = all_metrics_validation_set(targets, outputs, 512)\n",
    "        mse_0 += mses[0]\n",
    "        mse_1 += mses[1]\n",
    "        mse_2 += mses[2]\n",
    "        mse_3 += mses[3]\n",
    "        mse_4 += mses[4]\n",
    "        mse_5 += mses[5]\n",
    "        mse_6 += mses[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc322e1b-8868-4dba-895d-018e181cbf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/au2216/projects/FlowBench/codes/utils/visualization.py:57: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n"
     ]
    }
   ],
   "source": [
    "config_data = read_yaml_file('experiments/fno_1/', \"config.yaml\")\n",
    "FNO_trained_1 = TensorizedFNO(n_modes = config_data['n_modes'], in_channels = config_data['in_channels'], \n",
    "                              out_channels = config_data['out_channels'], hidden_channels = config_data['hidden_channels'], \n",
    "                              projection_channels = config_data['projection_channels'], n_layers = config_data['n_layers']).to(device)\n",
    "\n",
    "FNO_trained_1.load_checkpoint(save_name=\"200\", save_folder='experiments/fno_1/checkpoints')\n",
    "\n",
    "FNO_trained_1.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for idx in range(len(dataset)):\n",
    "        if idx>50:\n",
    "            break\n",
    "\n",
    "        id = 2999 - idx\n",
    "        inputs, targets = dataset[id]\n",
    "        outputs = FNO_trained_1.forward(inputs.unsqueeze(0).to(device))\n",
    "\n",
    "        plot_ldc_like(targets.unsqueeze(0).cpu().numpy(), outputs.cpu().numpy(), 0, os.path.join('experiments/fno_1/plots', f'FNO_1_{id}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "247f1275-e56a-4147-a0cd-458da0d9bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = read_yaml_file('experiments/fno_2/', \"config.yaml\")\n",
    "FNO_trained_2 = TensorizedFNO(n_modes = config_data['n_modes'], in_channels = config_data['in_channels'], \n",
    "                              out_channels = config_data['out_channels'], hidden_channels = config_data['hidden_channels'], \n",
    "                              projection_channels = config_data['projection_channels'], n_layers = config_data['n_layers']).to(device)\n",
    "\n",
    "FNO_trained_2.load_checkpoint(save_name=\"200\", save_folder='experiments/fno_2/checkpoints')\n",
    "\n",
    "FNO_trained_2.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for idx in range(len(dataset)):\n",
    "        if idx>50:\n",
    "            break\n",
    "            \n",
    "        id = 2999 - idx\n",
    "        inputs, targets = dataset[id]\n",
    "        outputs = FNO_trained_2.forward(inputs.unsqueeze(0).to(device))\n",
    "        \n",
    "        plot_ldc_like(targets.unsqueeze(0).cpu().numpy(), outputs.cpu().numpy(), 0, os.path.join('experiments/fno_2/plots', f'FNO_2_{id}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd33bf75-d872-4a23-866d-b4ddf298ba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4c62116-e031-41a3-a5d4-f656fc1b8458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"filtered_lrelu_plugin\"... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/au2216/projects/FlowBench/codes/utils/visualization.py:57: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n"
     ]
    }
   ],
   "source": [
    "config_data = read_yaml_file('experiments/cno_1/', \"config.yaml\")\n",
    "\n",
    "CNO_trained_1 = CompressedCNO(in_dim = config_data['in_dim'], out_dim = config_data['out_dim'], \n",
    "                          N_layers = config_data['N_layers'], in_size = config_data['in_size'], \n",
    "                          out_size = config_data['out_size']).to(device)\n",
    "\n",
    "CNO_trained_1.load_checkpoint(save_name=\"800\", save_folder='experiments/cno_1/checkpoints')\n",
    "\n",
    "CNO_trained_1.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for idx in range(len(dataset)):\n",
    "        if idx>50:\n",
    "            break\n",
    "\n",
    "        id = 2999 - idx\n",
    "        inputs, targets = dataset[id]\n",
    "        outputs = CNO_trained_1.forward(inputs.unsqueeze(0).to(device))\n",
    "        \n",
    "        plot_ldc_like(targets.unsqueeze(0).cpu().numpy(), outputs.cpu().numpy(), 0, os.path.join('experiments/cno_1/plots', f'CNO_1_{id}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b37590f8-7fd7-4b25-9887-5d35a0912648",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = read_yaml_file('experiments/cno_2/', \"config.yaml\")\n",
    "\n",
    "CNO_trained_2 = CompressedCNO(in_dim = config_data['in_dim'], out_dim = config_data['out_dim'], \n",
    "                          N_layers = config_data['N_layers'], in_size = config_data['in_size'], \n",
    "                          out_size = config_data['out_size']).to(device)\n",
    "\n",
    "CNO_trained_2.load_checkpoint(save_name=\"400\", save_folder='experiments/cno_2/checkpoints')\n",
    "\n",
    "CNO_trained_2.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for idx in range(len(dataset)):\n",
    "        if idx>50:\n",
    "            break\n",
    "\n",
    "        id = 2999 - idx\n",
    "        inputs, targets = dataset[id]\n",
    "        outputs = CNO_trained_2.forward(inputs.unsqueeze(0).to(device))\n",
    "        \n",
    "        plot_ldc_like(targets.unsqueeze(0).cpu().numpy(), outputs.cpu().numpy(), 0, os.path.join('experiments/cno_2/plots', f'CNO_2_{id}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b54fb9b-5b9c-4902-84d8-3df76dfd37dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.pth\n"
     ]
    }
   ],
   "source": [
    "!ls experiments/fno_1/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d2a47d7-dda3-4f5c-9a3b-be70296c7215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.pth\n"
     ]
    }
   ],
   "source": [
    "!ls experiments/fno_2/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54db9b67-66e3-42d7-bfc0-41d324ff8335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.pth  400.pth  600.pth  800.pth\n"
     ]
    }
   ],
   "source": [
    "!ls experiments/cno_1/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ca347cb-4684-4baf-9859-6e384353e340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.pth  400.pth\n"
     ]
    }
   ],
   "source": [
    "!ls experiments/cno_2/checkpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
